{"cells":[{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":4877,"status":"ok","timestamp":1685873533741,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"4pUIX5bUsmOv"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["import torch\n","import numpy as np\n","import torchvision\n","from torchvision import datasets, transforms\n","import torchvision.transforms as transforms\n","no_cuda = False\n","use_gpu = not no_cuda and torch.cuda.is_available()\n","device = torch.device(\"cuda\" if use_gpu else \"cpu\")\n","print(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"JSG_LFhO1xP-"},"source":["### Load dataset"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1454,"status":"ok","timestamp":1685873535188,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"8iNY2uJdLhD8","outputId":"f7ba6444-2b7b-485d-ae92-e258daa3f517"},"outputs":[{"name":"stdout","output_type":"stream","text":["Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100.0%\n"]},{"name":"stdout","output_type":"stream","text":["Extracting data\\FashionMNIST\\raw\\train-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100.0%\n"]},{"name":"stdout","output_type":"stream","text":["Extracting data\\FashionMNIST\\raw\\train-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100.0%\n"]},{"name":"stdout","output_type":"stream","text":["Extracting data\\FashionMNIST\\raw\\t10k-images-idx3-ubyte.gz to data\\FashionMNIST\\raw\n","\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n","Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"]},{"name":"stderr","output_type":"stream","text":["100.0%"]},{"name":"stdout","output_type":"stream","text":["Extracting data\\FashionMNIST\\raw\\t10k-labels-idx1-ubyte.gz to data\\FashionMNIST\\raw\n","\n","Train: 60000\n","Test: 10000\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["batch_size = 16\n","\n","trainset = datasets.FashionMNIST('data', train=True, download=True, transform=transforms.ToTensor())\n","trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n","    \n","testset = datasets.FashionMNIST('data', train=False, transform=transforms.ToTensor())\n","testloader = torch.utils.data.DataLoader(testset, batch_size=1, shuffle=False)\n","\n","print('Train: {}\\nTest: {}'.format(len(trainset), len(testset)))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"cNRAMwV511Hg"},"source":["### Define model"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1685873535189,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"wGqH43f2sv0N"},"outputs":[],"source":["import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 6, 3, padding=1)\n","        self.conv2 = nn.Conv2d(6, 16, 3, padding=0)\n","        self.relu = nn.ReLU()\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.fc1 = nn.Linear(16*6*6, 120)\n","        self.fc2 = nn.Linear(120, 84)\n","        self.fc3 = nn.Linear(84, 10)\n","        self.softmax = nn.Softmax(dim=1)\n","        \n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.pool(x)\n","        x = self.conv2(x)\n","        x = self.relu(x)\n","        x = self.pool(x)\n","        x = torch.flatten(x, 1)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        x = self.relu(x)\n","        x = self.fc3(x)\n","        x = self.softmax(x)\n","        return x\n","\n","model_to_quantize = Net().to(device)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"9sEA2YA9yZAC"},"source":["### Load pretrained model"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4668,"status":"ok","timestamp":1685873539854,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"wFR8ZbW-ci1q","outputId":"8c3c1dfb-87c5-4426-8f16-5ecfe96ee50e"},"outputs":[{"data":{"text/plain":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (relu): ReLU()\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=576, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n","  (softmax): Softmax(dim=1)\n",")"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["model_to_quantize.load_state_dict(torch.load('./model/mnist.pth'),strict=False)\n","model_to_quantize.eval()"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"I0hXFFUFfzIl"},"source":["## Quantize"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1685873539855,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"QdArc8Ni0LCp"},"outputs":[],"source":["from torch.ao.quantization import get_default_qconfig\n","from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n","from torch.ao.quantization import QConfigMapping"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ZQjOFSKFyQ-J"},"source":["### set quantization config and prepare model"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1685873539855,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"z4D-RaEK3Xjw"},"outputs":[],"source":["# set different quantization config\n","qconfig = get_default_qconfig('qnnpack')\n","\"\"\" (below is example of different configuration)\n","qconfig = get_default_qconfig(\"fbgemm\")\n","qconfig = torch.ao.quantization.default_qconfig\n","qconfig = torch.ao.quantization.qconfig.QConfig(\n","    activation=torch.ao.quantization.observer.HistogramObserver.with_args(\n","        qscheme=torch.per_tensor_symmetric, \n","        dtype=torch.qint8, \n","    ),\n","    weight=torch.ao.quantization.observer.PerChannelMinMaxObserver.with_args(\n","        #ch_axis=1,  \n","        qscheme=torch.per_channel_symmetric,\n","        dtype=torch.qint8,\n","        ))\n","\"\"\"\n","qconfig_mapping = QConfigMapping().set_global(qconfig)"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1685873539856,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"6yJhARgR3a-a","outputId":"12d40fe0-0b6a-4d65-91a7-9cb4babaec20"},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\GIGABYTE\\anaconda3\\envs\\AI\\lib\\site-packages\\torch\\ao\\quantization\\fx\\utils.py:829: UserWarning: QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize for fixed qparams ops, ignoring QConfig(activation=functools.partial(<class 'torch.ao.quantization.observer.HistogramObserver'>, reduce_range=False){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x000001DD84D89AF0>}, weight=functools.partial(<class 'torch.ao.quantization.observer.MinMaxObserver'>, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric){'factory_kwargs': <function _add_module_to_qconfig_obs_ctr.<locals>.get_factory_kwargs_based_on_module_device at 0x000001DD84D89AF0>}).\n","Please use torch.ao.quantization.get_default_qconfig_mapping or torch.ao.quantization.get_default_qat_qconfig_mapping. Example:\n","    qconfig_mapping = get_default_qconfig_mapping(\"fbgemm\")\n","    model = prepare_fx(model, qconfig_mapping, example_inputs)\n","  warnings.warn((\"QConfig must specify a FixedQParamsObserver or a FixedQParamsFakeQuantize \"\n"]}],"source":["example_inputs = (next(iter(trainloader))[0]) # to know model input data type\n","prepared_model = prepare_fx(model_to_quantize, qconfig_mapping, example_inputs) # prepare to quantize model (fuse module (ex:CONV+BN+RELU...)，insert observer)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"U062-zl9xKJa"},"source":["### calibration (use representation data)"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":3053,"status":"ok","timestamp":1685873542905,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"0Anq--qu7gcR"},"outputs":[],"source":["def calibrate(model, device, data_loader):\n","  model.to(device)\n","  model.eval()\n","  with torch.no_grad():\n","      for data, target in data_loader:\n","        data, target = data.to(device), target.to(device) #device\n","        model(data)\n","calibrate(prepared_model, 'cpu', testloader)  # run calibration on sample data"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1685873542906,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"Psc04GKDxnmZ"},"outputs":[],"source":["quantized_model = convert_fx(prepared_model) # convert the calibrated model to a quantized model"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"2rKhMe7Cf2El"},"source":["### check quantized model"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1685873542906,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"5L0J1QtsxDKb","outputId":"778b929a-02ed-4924-f202-53d63896c979"},"outputs":[{"name":"stdout","output_type":"stream","text":["Net(\n","  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n","  (relu): ReLU()\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (fc1): Linear(in_features=576, out_features=120, bias=True)\n","  (fc2): Linear(in_features=120, out_features=84, bias=True)\n","  (fc3): Linear(in_features=84, out_features=10, bias=True)\n","  (softmax): Softmax(dim=1)\n",")\n"]}],"source":["print(model_to_quantize)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1685873542906,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"9yzeK1UeOExy","outputId":"0b125913-5e99-4ce7-bc58-8f5d52385df9"},"outputs":[{"name":"stdout","output_type":"stream","text":["GraphModule(\n","  (conv1): QuantizedConvReLU2d(1, 6, kernel_size=(3, 3), stride=(1, 1), scale=0.008702474646270275, zero_point=0, padding=(1, 1))\n","  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv2): QuantizedConvReLU2d(6, 16, kernel_size=(3, 3), stride=(1, 1), scale=0.021146273240447044, zero_point=0)\n","  (fc1): QuantizedLinearReLU(in_features=576, out_features=120, scale=0.039127349853515625, zero_point=0, qscheme=torch.per_tensor_affine)\n","  (fc2): QuantizedLinearReLU(in_features=120, out_features=84, scale=0.0579749271273613, zero_point=0, qscheme=torch.per_tensor_affine)\n","  (fc3): QuantizedLinear(in_features=84, out_features=10, scale=0.22861747443675995, zero_point=175, qscheme=torch.per_tensor_affine)\n","  (softmax): Softmax(dim=1)\n",")\n","\n","\n","\n","def forward(self, x):\n","    conv1_input_scale_0 = self.conv1_input_scale_0\n","    conv1_input_zero_point_0 = self.conv1_input_zero_point_0\n","    quantize_per_tensor = torch.quantize_per_tensor(x, conv1_input_scale_0, conv1_input_zero_point_0, torch.quint8);  x = conv1_input_scale_0 = conv1_input_zero_point_0 = None\n","    conv1 = self.conv1(quantize_per_tensor);  quantize_per_tensor = None\n","    pool = self.pool(conv1);  conv1 = None\n","    conv2 = self.conv2(pool);  pool = None\n","    pool_1 = self.pool(conv2);  conv2 = None\n","    flatten = torch.flatten(pool_1, 1);  pool_1 = None\n","    fc1 = self.fc1(flatten);  flatten = None\n","    fc2 = self.fc2(fc1);  fc1 = None\n","    fc3 = self.fc3(fc2);  fc2 = None\n","    dequantize_8 = fc3.dequantize();  fc3 = None\n","    softmax = self.softmax(dequantize_8);  dequantize_8 = None\n","    return softmax\n","    \n","# To see more debug info, please use `graph_module.print_readable()`\n"]}],"source":["print(quantized_model)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"qn6H7dcWf54N"},"source":["### performance analysis"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1685873542907,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"zFV1z6Yj5Vkd"},"outputs":[],"source":["import os\n","def print_size_of_model(model):\n","    \"\"\" Print the size of the model.\n","    \n","    Args:\n","        model: model whose size needs to be determined\n","\n","    \"\"\"\n","    torch.save(model.state_dict(), \"temp.p\")\n","    print('Size of the model(MB):', os.path.getsize(\"temp.p\")/1e6)\n","    os.remove('temp.p')"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1685873542907,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"zM6QafMp7GW3"},"outputs":[],"source":["def compare(model, device, test_loader, quantize=False):\n","  model.to(device)\n","  model.eval()\n","\n","  total = 0\n","  correct = 0\n","  with torch.no_grad():\n","    for data in test_loader:\n","      images, labels = data\n","      images, labels = images.to(device),labels.to(device)\n","      outputs = model(images)\n","      # the class with the highest energy is what we choose as prediction\n","      _, predicted = torch.max(outputs.data, 1)\n","      total += labels.size(0)\n","      correct += (predicted == labels).sum().item()\n","\n","  test_loss = 0\n","  \n","  print(\"========================================= PERFORMANCE =============================================\")\n","  print_size_of_model(model)\n","  print('\\nAccuracy: {}/{} ({:.0f}%)\\n'.format( correct, total,100. * correct / total))\n","  print(\"====================================================================================================\") "]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2411,"status":"ok","timestamp":1685873545312,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"2SkQAJmZabxM","outputId":"d980c0f2-f140-44c4-ec50-114336f49851"},"outputs":[{"name":"stdout","output_type":"stream","text":["========================================= PERFORMANCE =============================================\n","Size of the model(MB): 0.327587\n","\n","Accuracy: 9048/10000 (90%)\n","\n","====================================================================================================\n"]}],"source":["device = 'cpu'\n","compare(model=model_to_quantize, device=device, test_loader=testloader)"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3343,"status":"ok","timestamp":1685873548653,"user":{"displayName":"沈桓敬 SHEN, HUAN-JING P76114757","userId":"12290812438432293375"},"user_tz":-480},"id":"YqciaigbddX9","outputId":"f23ab1ac-e5d4-4e2d-ae11-77208964205a"},"outputs":[{"name":"stdout","output_type":"stream","text":["========================================= PERFORMANCE =============================================\n","Size of the model(MB): 0.088283\n","\n","Accuracy: 9024/10000 (90%)\n","\n","====================================================================================================\n"]}],"source":["device = 'cpu'\n","compare(model=quantized_model, device=device, test_loader=testloader)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Extract layer name"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['x', 'quantize_per_tensor', 'conv1', 'pool', 'conv2', 'pool_1', 'flatten', 'fc1', 'fc2', 'fc3', 'dequantize', 'softmax']\n"]}],"source":["from torchvision.models.feature_extraction import get_graph_node_names\n","\n","train_nodes, eval_nodes = get_graph_node_names(quantized_model)\n","print(eval_nodes)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Extract train weight"]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[-0.2616,  0.1744,  0.2791],\n","         [-0.2093, -0.2616,  0.2268],\n","         [ 0.4535, -0.3314, -0.1221]]], size=(1, 3, 3), dtype=torch.qint8,\n","       quantization_scheme=torch.per_tensor_affine, scale=0.017443126067519188,\n","       zero_point=0)\n","tensor([[[ 0.4361, -0.2093, -0.2442],\n","         [ 0.4186, -0.0174, -0.3489],\n","         [ 0.3489,  0.2268, -0.3663]]], size=(1, 3, 3), dtype=torch.qint8,\n","       quantization_scheme=torch.per_tensor_affine, scale=0.017443126067519188,\n","       zero_point=0)\n","tensor([[[-1.0117,  0.0000,  0.1221],\n","         [-2.2327, -1.0291, -0.3489],\n","         [-0.9768, -0.6454,  0.2616]]], size=(1, 3, 3), dtype=torch.qint8,\n","       quantization_scheme=torch.per_tensor_affine, scale=0.017443126067519188,\n","       zero_point=0)\n","tensor([[[ 0.4361,  0.2442, -0.2093],\n","         [ 0.5407,  0.2093,  0.1047],\n","         [ 0.0872,  0.2616,  0.5407]]], size=(1, 3, 3), dtype=torch.qint8,\n","       quantization_scheme=torch.per_tensor_affine, scale=0.017443126067519188,\n","       zero_point=0)\n","tensor([[[ 0.2442, -0.3837,  0.1570],\n","         [ 0.5582, -0.4361, -0.4710],\n","         [ 0.1047,  0.2791, -0.4361]]], size=(1, 3, 3), dtype=torch.qint8,\n","       quantization_scheme=torch.per_tensor_affine, scale=0.017443126067519188,\n","       zero_point=0)\n","tensor([[[ 0.4710,  0.4012, -0.1570],\n","         [ 0.2791,  0.2965,  0.1919],\n","         [-0.1221,  0.4535, -0.1919]]], size=(1, 3, 3), dtype=torch.qint8,\n","       quantization_scheme=torch.per_tensor_affine, scale=0.017443126067519188,\n","       zero_point=0)\n"]}],"source":["# Change layer name to get each layer value\n","for i in quantized_model.conv1.weight():\n","  print(i)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Extract input activation"]},{"cell_type":"code","execution_count":18,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[[[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   2,   0,   1,\n","            30,   0,   0,   2,   0,   0,   0,   0,   1,   1,   0,   0,   1,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,   0,  39,\n","           122,  57,   0,   0,   2,   1,   1,   1,   0,   1,   0,   0,   0,   1],\n","          [  0,   0,   0,   0,   0,   1,   0,   0,   2,   1,   0,   1,  30,  53,\n","            78, 164,  90,   0,   0,   0,   0,   0,   0,   2,   5,   9,   4,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   1,   0,   0,   0,  30,  68,  58,\n","            45,  65, 122,  93,  37,  12,   0,   0,   6,  31,  45,  71,  43,   0],\n","          [  0,   0,   0,   0,   1,   5,  19,  57,  17,   0,   0,  56,  80,  45,\n","            54,  53,  56,  69,  90,  80,  71,  65,  60,  49,  53,  86,  50,   0],\n","          [  0,   0,   1,   1,   9,  57,  49,  43, 128,   8,   0,   2,  64,  67,\n","            80,  61,  65,  60,  57,  45,  37,  32,  34,  68,  72,  94,  49,   0],\n","          [  0,   0,   0,   0,  16,  57,  45,  26,  79, 130,  47,   0,  35,  68,\n","            86,  95,  71,  43, 120, 241, 139, 101, 197, 117,  93, 104,  67,   0],\n","          [  0,   0,   0,   0,   2,  56,  39,  37,  45,  97, 141, 116, 119,  95,\n","            41,  34,  43,  58, 135, 227, 196, 187, 239,  79,  49,  83,  74,   0],\n","          [  0,   0,   1,   0,   4,  71,  32,  37,  45,  45,  69, 128, 100, 120,\n","           132, 123, 135, 171, 179, 161, 127, 122, 183, 100,  39,  68,  76,   0],\n","          [  0,   0,   1,   0,   1,  76,  47,  28,  47,  74,  38,  52,  58,  43,\n","            98, 122, 146, 228, 165, 159, 178, 164, 179, 246,  86,  56,  82,   1],\n","          [  0,   0,   0,   0,   1,  89, 157, 165, 171,  93,  52,  56,  60,  83,\n","            82,  58,  89, 157, 174, 201, 198, 178, 200, 130,  67,  78,  75,   1],\n","          [  0,   2,   1,   0,   4,  60,  49,  71,  64,  64,  69,  79,  91,  54,\n","            31,  58,  75,  54,  57,  52, 115, 254, 132,  34,  63,  74,  72,   0],\n","          [  1,   0,   0,   0,   4,  85,  74,  67,  68,  69,  74,  80,  94,  56,\n","            52,  85, 104, 105,  87,  69,  63, 141,  93,  87,  89, 101,  85,   2],\n","          [ 26,  16,   0,   2,   0,  79, 115,  89, 100,  98, 100, 100, 108,  86,\n","            76,  83,  93, 119, 112, 112, 112,  94, 102,  91,  75,  41,  16,  20],\n","          [ 12,  56,  42,  35,  16,  31,  58,  50,  53,  54,  57,  60,  64,  57,\n","            38,  26,  16,  17,   8,  24,  23,  35,  31,  13,  24,  45,  58,  28],\n","          [  0,  16,  43,  49,  80,  74,  65,  60,  54,  56,  54,  52,  50,  56,\n","            56,  68,  74,  82,  82,  78,  76,  75,  83,  91,  94,  91,  58,   0],\n","          [  0,   0,   0,   1,   4,  21,  41,  47,  47,  47,  50,  56,  57,  60,\n","            61,  56,  47,  46,  43,  42,  42,  39,  39,  32,  21,   4,   0,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n","          [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n","             0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0]]]],\n","       dtype=torch.uint8)\n","torch.Size([1, 1, 28, 28])\n"]}],"source":["import torch\n","from torchvision.models.feature_extraction import create_feature_extractor\n","# Change layer name to get each layer value\n","model = create_feature_extractor(quantized_model, [\"quantize_per_tensor\"])\n","for data in testloader:\n","      images, labels = data\n","      continue\n","\n","\n","torch.set_printoptions(profile='full')\n","outputs = model(images)\n","\n","print(outputs['quantize_per_tensor'].int_repr())\n","\n","with open('quantize_per_tensor.txt', 'a') as f:\n","    for k, v in outputs.items():\n","        print(v.shape)\n","        for i in range(len(v)):\n","             for j in range(len(v[i])):\n","                  for k in range(len(v[i][j])):\n","                        for l in range(len(v[i][j][k])):\n","                              #print(v[i][j][k][l].int_repr())\n","                              f.write(str(bin(v[i][j][k][l].int_repr().numpy().tolist()))[2:].zfill(8))\n","                              f.write(\"\\n\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.0"}},"nbformat":4,"nbformat_minor":0}
